#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è³‡æ–™è™•ç†è…³æœ¬ï¼šå°‡ AML äº¤æ˜“è³‡æ–™è½‰æ›ç‚ºåœ–çµæ§‹ä¸¦å„²å­˜ç‚º .pt æª”æ¡ˆ
å‰‡ node_gfp_features çš„æœ€çµ‚ç¶­åº¦æ˜¯:
    - å‰åŠéƒ¨åˆ†ï¼šè©²å¸³æˆ¶ä½œç‚ºä¾†æºçš„æ‰€æœ‰äº¤æ˜“çš„ edge_gfp_featureså¹³å‡å€¼ã€‚
    - å¾ŒåŠéƒ¨åˆ†ï¼šè©²å¸³æˆ¶ä½œç‚ºç›®çš„çš„æ‰€æœ‰äº¤æ˜“çš„ edge_gfp_featureså¹³å‡å€¼ã€‚

"""

import os
import json
import torch
import pandas as pd
import re
import numpy as np
from datetime import datetime
from tqdm import tqdm
from torch_geometric.data import Data
from snapml import GraphFeaturePreprocessor


def normalize_account_id(bank_code, account):
    """
    æ¨™æº–åŒ–å¸³æˆ¶ IDï¼Œç§»é™¤å‰å°é›¶
    
    Args:
        bank_code: éŠ€è¡Œä»£ç¢¼
        account: å¸³è™Ÿ
    
    Returns:
        str: æ¨™æº–åŒ–å¾Œçš„å¸³æˆ¶ ID
    """
    # å°‡éŠ€è¡Œä»£ç¢¼å’Œå¸³è™Ÿè½‰ç‚ºå­—ä¸²ä¸¦ç§»é™¤å‰å°é›¶
    bank_code_str = str(bank_code).lstrip('0') or '0'
    account_str = str(account).lstrip('0') or '0'
    return f"{bank_code_str}_{account_str}"


def parse_patterns_file(patterns_path):
    """
    è§£æ Patterns.txt æª”æ¡ˆï¼Œæå–æ´—éŒ¢äº¤æ˜“è³‡è¨Š
    
    Args:
        patterns_path: Patterns.txt æª”æ¡ˆè·¯å¾‘
    
    Returns:
        list: æ´—éŒ¢äº¤æ˜“åˆ—è¡¨ï¼Œæ¯ç­†äº¤æ˜“ç‚º dict
        set: æ´—éŒ¢ç›¸é—œå¸³æˆ¶é›†åˆï¼ˆå·²æ¨™æº–åŒ–ï¼Œç§»é™¤å‰å°é›¶ï¼‰
    """
    laundering_transactions = []
    laundering_accounts = set()
    
    with open(patterns_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ‰¾å‡ºæ‰€æœ‰æ´—éŒ¢å€å¡Š
    pattern_blocks = re.findall(
        r'BEGIN LAUNDERING ATTEMPT.*?END LAUNDERING ATTEMPT[^\n]*',
        content,
        re.DOTALL
    )
    
    for block in pattern_blocks:
        lines = block.strip().split('\n')
        for line in lines[1:-1]:  # è·³é BEGIN å’Œ END è¡Œ
            if line.strip() and not line.startswith('BEGIN') and not line.startswith('END'):
                parts = line.strip().split(',')
                if len(parts) >= 10:
                    from_bank = parts[1].strip()
                    from_account = parts[2].strip()
                    to_bank = parts[3].strip()
                    to_account = parts[4].strip()
                    
                    transaction = {
                        'timestamp': parts[0].strip(),
                        'from_bank': from_bank,
                        'from_account': from_account,
                        'to_bank': to_bank,
                        'to_account': to_account,
                        'amount_received': float(parts[5]),
                        'receiving_currency': parts[6].strip(),
                        'amount_paid': float(parts[7]),
                        'payment_currency': parts[8].strip(),
                        'payment_format': parts[9].strip(),
                        'is_laundering': int(parts[10].strip())
                    }
                    laundering_transactions.append(transaction)
                    
                    # æ¨™æº–åŒ–å¸³æˆ¶ IDï¼ˆç§»é™¤å‰å°é›¶ï¼‰
                    from_acct = normalize_account_id(from_bank, from_account)
                    to_acct = normalize_account_id(to_bank, to_account)
                    laundering_accounts.add(from_acct)
                    laundering_accounts.add(to_acct)
    
    return laundering_transactions, laundering_accounts


def transaction_hop(trans_data, acct_list, hop=1, without_acct_list=None):
    """
    å–å¾—è­¦ç¤ºå¸³æˆ¶çš„ n-hop åå–®
    
    Args:
        trans_data: äº¤æ˜“è³‡æ–™ DataFrame
        acct_list: å¸³æˆ¶åˆ—è¡¨
        hop: è·³æ•¸
        without_acct_list: è¦æ’é™¤çš„å¸³æˆ¶åˆ—è¡¨
    
    Returns:
        set: n-hop å¸³æˆ¶é›†åˆ
    """
    acct_hop_dict = set(acct_list)

    for _ in range(hop):
        related_transaction = trans_data[
            (trans_data['from_acct'].isin(acct_hop_dict)) | 
            (trans_data['to_acct'].isin(acct_hop_dict))
        ]
        
        for row in related_transaction.itertuples():
            acct_hop_dict.add(row.from_acct)
            acct_hop_dict.add(row.to_acct)
    
    if without_acct_list is not None:
        acct_hop_dict = acct_hop_dict - set(without_acct_list)

    return acct_hop_dict


def datetime_to_timestamp(timestamp_str):
    """
    å°‡æ™‚é–“æˆ³å­—ä¸²è½‰æ›ç‚ºæ•¸å€¼ï¼ˆUnix timestamp æˆ–åºæ•¸ï¼‰
    
    Args:
        timestamp_str: æ™‚é–“æˆ³å­—ä¸²ï¼Œæ ¼å¼ 'YYYY/MM/DD HH:MM'
    
    Returns:
        float: æ™‚é–“æˆ³æ•¸å€¼
    """
    try:
        dt = datetime.strptime(timestamp_str, '%Y/%m/%d %H:%M')
        # è½‰æ›ç‚º Unix timestampï¼ˆç§’ï¼‰
        return dt.timestamp()
    except:
        return 0.0

def extract_gfp_features(trans_data, acct_id, time_window_days=1):
    """
    ä½¿ç”¨ Graph Feature Preprocessor æå–åœ–ç‰¹å¾µ
    
    Args:
        trans_data: è™•ç†å¾Œçš„äº¤æ˜“è³‡æ–™ DataFrame
        acct_id: å¸³æˆ¶ ID æ˜ å°„å­—å…¸
        time_window_days: æ™‚é–“çª—å£ï¼ˆå¤©ï¼‰
    
    Returns:
        node_features: ç¯€é»å±¤ç´šç‰¹å¾µ (num_nodes, num_features)
        edge_features: äº¤æ˜“å±¤ç´šç‰¹å¾µ (num_edges, num_features)
    """
    print(f"\n{'='*70}")
    print("ä½¿ç”¨ Graph Feature Preprocessor æå–åœ–ç‰¹å¾µ")
    print(f"{'='*70}")
    
    # æº–å‚™ GFP è¼¸å…¥æ ¼å¼ï¼š[transaction_id, source_account_id, target_account_id, timestamp, amount]
    # GFP éœ€è¦çš„æ˜¯æ•¸å€¼å‹çš„ account ID
    gfp_input = np.column_stack([
        np.arange(len(trans_data)),           # transaction_id
        trans_data['from_acct_id'].values,    # source_account_id
        trans_data['to_acct_id'].values,      # target_account_id  
        trans_data['txn_timestamp'].values,   # timestamp
        trans_data['txn_amt'].values          # amount (ç”¨æ–¼çµ±è¨ˆ)
    ])
    
    print(f"GFP è¼¸å…¥å½¢ç‹€: {gfp_input.shape}")
    print(f"äº¤æ˜“æ•¸é‡: {len(gfp_input)}")
    print(f"å¸³æˆ¶æ•¸é‡: {len(acct_id)}")
    
    # è¨ˆç®—æ™‚é–“çª—å£ï¼ˆç§’ï¼‰
    time_window_seconds = time_window_days * 24 * 3600
    
    # é…ç½® GFP åƒæ•¸
    params = {
        "num_threads": 4,
        "time_window": time_window_seconds,
        
        # ç¯€é»çµ±è¨ˆç‰¹å¾µ
        "vertex_stats": True,
        "vertex_stats_cols": [4],  # ä½¿ç”¨ amount æ¬„ä½è¨ˆç®—çµ±è¨ˆ
        "vertex_stats_feats": [0, 1, 2, 3, 4, 8, 9, 10],  # fan, deg, ratio, avg, sum, var, skew, kurtosis
        
        # Fan-in/out patterns
        "fan": True,
        "fan_tw": time_window_seconds,
        "fan_bins": [2, 5, 10],
        
        # Degree patterns  
        "degree": True,
        "degree_tw": time_window_seconds,
        "degree_bins": [2, 5, 10],
        
        # Scatter-gather patterns
        "scatter-gather": True,
        "scatter-gather_tw": time_window_seconds,
        "scatter-gather_bins": [2, 5],
        
        # Temporal cycle patterns
        "temp-cycle": True,
        "temp-cycle_tw": time_window_seconds,
        "temp-cycle_bins": [2, 5],
        
        # Length-constrained simple cycle (optional, å¯èƒ½è¼ƒæ…¢)
        "lc-cycle": False,
    }
    
    print("\nGFP åƒæ•¸é…ç½®:")
    print(f"  æ™‚é–“çª—å£: {time_window_days} å¤© ({time_window_seconds} ç§’)")
    print(f"  ç¯€é»çµ±è¨ˆ: ä½¿ç”¨ amount æ¬„ä½")
    print(f"  æ¨¡å¼æª¢æ¸¬: fan, degree, scatter-gather, temporal-cycle")
    
    # åˆå§‹åŒ–ä¸¦åŸ·è¡Œ GFP
    print("\né–‹å§‹æå–åœ–ç‰¹å¾µ...")
    gfp = GraphFeaturePreprocessor()
    gfp.set_params(params)
    
    # fit_transform æœƒåŒæ™‚å»ºç«‹åœ–ä¸¦æå–ç‰¹å¾µ
    enriched_features = gfp.fit_transform(gfp_input.astype("float64"))
    
    print(f"âœ“ ç‰¹å¾µæå–å®Œæˆï¼")
    print(f"  åŸå§‹ç‰¹å¾µæ•¸: {gfp_input.shape[1]}")
    print(f"  ç¸½ç‰¹å¾µæ•¸: {enriched_features.shape[1]}")
    print(f"  æ–°å¢ GFP ç‰¹å¾µæ•¸: {enriched_features.shape[1] - gfp_input.shape[1]}")
    
    # åˆ†é›¢åŸå§‹ç‰¹å¾µå’Œ GFP ç‰¹å¾µ
    gfp_features_only = enriched_features[:, gfp_input.shape[1]:]  # åªè¦æ–°å¢çš„ç‰¹å¾µ
    
    # äº¤æ˜“å±¤ç´šç‰¹å¾µ = æ¯æ¢ edge çš„ GFP ç‰¹å¾µ
    edge_features = gfp_features_only
    
    # ç¯€é»å±¤ç´šç‰¹å¾µ = èšåˆè©²ç¯€é»æ‰€æœ‰ç›¸é—œäº¤æ˜“çš„ GFP ç‰¹å¾µ
    # ç‚ºæ¯å€‹ç¯€é»è¨ˆç®—å…¶ä½œç‚º source å’Œ target çš„å¹³å‡ç‰¹å¾µ
    num_nodes = len(acct_id)
    num_gfp_features = gfp_features_only.shape[1]
    node_features = np.zeros((num_nodes, num_gfp_features * 2))  # *2 for in/out
    """
    print("\nèšåˆç¯€é»å±¤ç´šç‰¹å¾µ...")
    for node_id in range(num_nodes):
        # ä½œç‚º source çš„äº¤æ˜“
        src_mask = trans_data['from_acct_id'] == node_id
        if src_mask.any():
            node_features[node_id, :num_gfp_features] = gfp_features_only[src_mask].mean(axis=0)
        
        # ä½œç‚º target çš„äº¤æ˜“
        tgt_mask = trans_data['to_acct_id'] == node_id
        if tgt_mask.any():
            node_features[node_id, num_gfp_features:] = gfp_features_only[tgt_mask].mean(axis=0)
    
    print(f"âœ“ ç¯€é»ç‰¹å¾µèšåˆå®Œæˆ")
    print(f"  ç¯€é»ç‰¹å¾µç¶­åº¦: {node_features.shape}")
    """
    print(f"  äº¤æ˜“ç‰¹å¾µç¶­åº¦: {edge_features.shape}")
    print(f"{'='*70}\n")
    
    return node_features, edge_features

def main():
    # è¨­å®šè·¯å¾‘
    base_path = "./data/HI_Small"  # è«‹ä¿®æ”¹ç‚ºä½ çš„è³‡æ–™è·¯å¾‘
    output_path = "./data/HI_Small/results"  # è«‹ä¿®æ”¹ç‚ºè¼¸å‡ºè·¯å¾‘
    
    trans_path = os.path.join(base_path, "HI-Small_Trans.csv")
    patterns_path = os.path.join(base_path, "HI-Small_Patterns.txt")
    
    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    os.makedirs(output_path, exist_ok=True)
    
    print("è¼‰å…¥è³‡æ–™...")
    # è¼‰å…¥äº¤æ˜“è³‡æ–™
    trans_data = pd.read_csv(trans_path)
    print(f"äº¤æ˜“è³‡æ–™: {len(trans_data)} ç­†")
    
    # è§£æ Patterns æª”æ¡ˆ
    print("è§£ææ´—éŒ¢æ¨¡å¼...")
    laundering_transactions, laundering_accounts = parse_patterns_file(patterns_path)
    print(f"æ´—éŒ¢äº¤æ˜“: {len(laundering_transactions)} ç­†")
    print(f"æ´—éŒ¢ç›¸é—œå¸³æˆ¶: {len(laundering_accounts)} å€‹")
    
    # 1. è™•ç†å¸³æˆ¶è³‡è¨Šï¼šçµ„åˆéŠ€è¡Œä»£ç¢¼å’Œå¸³è™Ÿï¼ˆç§»é™¤å‰å°é›¶ï¼‰
    print("\nè™•ç†å¸³æˆ¶è³‡è¨Š...")
    trans_data['from_acct'] = trans_data.apply(
        lambda row: normalize_account_id(row['From Bank'], row['Account']), 
        axis=1
    )
    trans_data['to_acct'] = trans_data.apply(
        lambda row: normalize_account_id(row['To Bank'], row['Account.1']), 
        axis=1
    )
    
    # 2. è½‰æ›å¸³æˆ¶ä»£è™Ÿç‚º ID ç·¨è™Ÿ
    print("è™•ç†å¸³æˆ¶ ID æ˜ å°„...")
    acct_list = pd.concat([trans_data['from_acct'], trans_data['to_acct']]).unique()
    
    acct_id = {}
    for i in acct_list:
        acct_id[i] = len(acct_id)
    
    trans_data['from_acct_id'] = trans_data['from_acct'].map(lambda x: acct_id[x])
    trans_data['to_acct_id'] = trans_data['to_acct'].map(lambda x: acct_id[x])
    
    # 3. è™•ç†äº¤æ˜“æ™‚é–“ï¼ˆè½‰æ›ç‚ºæ•¸å€¼ï¼‰
    print("è™•ç†äº¤æ˜“æ™‚é–“...")
    trans_data['txn_timestamp'] = trans_data['Timestamp'].apply(datetime_to_timestamp)
    
    # 4. è™•ç†äº¤æ˜“é‡‘é¡ï¼ˆä½¿ç”¨ Amount Paidï¼Œè³‡æ–™é›†å·²ç¶“æ˜¯ç¾å…ƒï¼‰
    print("è™•ç†äº¤æ˜“é‡‘é¡...")
    trans_data['txn_amt'] = trans_data['Amount Paid']
    
    # 5. è™•ç†è²¨å¹£é¡å‹ï¼ˆcurrency_type ä½¿ç”¨ Payment Currencyï¼‰
    print("è™•ç†è²¨å¹£é¡å‹...")
    currency_id = {}
    for i in trans_data['Payment Currency'].unique():
        currency_id[i] = len(currency_id)
    
    trans_data['currency_id'] = trans_data['Payment Currency'].map(
        lambda x: currency_id[x]
    )
    
    # 6. è™•ç†äº¤æ˜“é€šè·¯ï¼ˆchannel_type ä½¿ç”¨ Payment Formatï¼‰
    print("è™•ç†äº¤æ˜“é€šè·¯...")
    channel_id = {}
    for i in trans_data['Payment Format'].unique():
        channel_id[i] = len(channel_id)
    
    trans_data['channel_id'] = trans_data['Payment Format'].map(
        lambda x: channel_id[x]
    )
    
    # 7. åˆ¤æ–·æ˜¯å¦ç‚ºè‡ªæˆ‘äº¤æ˜“ï¼ˆæ ¹æ“šéŠ€è¡Œä»£ç¢¼æ˜¯å¦ç›¸åŒï¼‰
    print("è™•ç†è‡ªæˆ‘äº¤æ˜“æ¨™è¨˜...")
    trans_data['is_self_txn_id'] = (
        trans_data['From Bank'].astype(str).str.lstrip('0') == 
        trans_data['To Bank'].astype(str).str.lstrip('0')
    ).astype(int)
    
    # å»ºæ§‹åœ–çµæ§‹
    print("\nå»ºæ§‹åœ–çµæ§‹...")
    
    # Edge index
    from_acct_id = trans_data['from_acct_id'].values
    to_acct_id = trans_data['to_acct_id'].values
    edge_index = torch.tensor([from_acct_id, to_acct_id], dtype=torch.long)
    
    # Edge attributes: [is_self_txn, txn_amt, currency_id, channel_id, timestamp]
    is_self = trans_data['is_self_txn_id'].values
    amt = trans_data['txn_amt'].values
    currency = trans_data['currency_id'].values
    channel = trans_data['channel_id'].values
    timestamp = trans_data['txn_timestamp'].values
    
    edge_attr = torch.tensor(
        [is_self, amt, currency, channel, timestamp], 
        dtype=torch.float
    ).t()
    
    # ===== ä½¿ç”¨ GFP æå–åœ–ç‰¹å¾µ =====
    node_gfp_features, edge_gfp_features = extract_gfp_features(
        trans_data, acct_id, time_window_days=1
    )
    
    # è½‰æ›ç‚º torch tensor
    edge_gfp_features = torch.tensor(edge_gfp_features, dtype=torch.float)
    
    # ğŸ”§ æ–°å¢ï¼šæ•´åˆ GFP ç‰¹å¾µåˆ° edge_attr
    edge_attr = torch.cat([edge_attr, edge_gfp_features], dim=1)
    
    print(f"\n{'='*70}")
    print("Edge Attributes æ•´åˆå®Œæˆ")
    print(f"{'='*70}")
    print(f"  åŸå§‹ç‰¹å¾µæ•¸: 5 (is_self_txn, txn_amt, currency_id, channel_id, timestamp)")
    print(f"  GFP ç‰¹å¾µæ•¸: {edge_gfp_features.shape[1]}")
    print(f"  æ•´åˆå¾Œç¸½ç‰¹å¾µæ•¸: {edge_attr.shape[1]}")
    print(f"  Edge_attr å½¢ç‹€: {edge_attr.shape}")
    print(f"{'='*70}\n")
    
    # Labels
    print("è™•ç†æ¨™ç±¤...")
    
    # å¾ laundering_accounts ä¸­æå–åœ¨ acct_id ä¸­çš„å¸³æˆ¶
    alert_accts = [acct for acct in laundering_accounts if acct in acct_id]
    
    # å–å¾—è­¦ç¤ºå¸³æˆ¶çš„ 1-hop é„°å±…ï¼ˆæ’é™¤è­¦ç¤ºå¸³æˆ¶æœ¬èº«ï¼‰
    print("è¨ˆç®— 1-hop é„°å±…...")
    acct_nhop_with_alert = transaction_hop(
        trans_data,
        alert_accts, 
        hop=1, 
        without_acct_list=alert_accts
    )
    
    labels = [-1 for i in range(len(acct_id))]
    
    # è¨­å®šè­¦ç¤ºå¸³æˆ¶æ¨™ç±¤ç‚º 1
    for acct in alert_accts:
        if acct in acct_id:
            labels[acct_id[acct]] = 1
    
    # è¨­å®š 1-hop é„°å±…æ¨™ç±¤ç‚º 0ï¼ˆæ­£å¸¸å¸³æˆ¶ï¼‰
    for acct in acct_nhop_with_alert:
        if acct in acct_id:
            labels[acct_id[acct]] = 0
    
    y = torch.tensor(labels, dtype=torch.long)
    
    print(f"æ¨™ç±¤çµ±è¨ˆ - è­¦ç¤º(1): {labels.count(1)}, æ­£å¸¸(0): {labels.count(0)}, æœªçŸ¥(-1): {labels.count(-1)}")
    

    # å»ºç«‹ Data ç‰©ä»¶
    data = Data(
        edge_index=edge_index, 
        edge_attr=edge_attr,  # å·²åŒ…å« GFP ç‰¹å¾µ
        y=y
    )
    # æ³¨æ„ï¼šnode_gfp_features ä¸å†éœ€è¦ï¼Œå› ç‚ºæˆ‘å€‘ä½¿ç”¨ edge-level GFP
    
    print(f"\nåœ–çµæ§‹: {data}")
    print(f"ç¯€é»æ•¸é‡: {len(acct_id)}")
    print(f"é‚Šæ•¸é‡: {len(edge_index[0])}")
    print(f"é‚Šç‰¹å¾µç¶­åº¦: {edge_attr.shape}")
    
    # å„²å­˜çµæœ
    print("\nå„²å­˜çµæœ...")
    
    acct_id_path = os.path.join(output_path, 'acct_id.json')
    data_path = os.path.join(base_path, 'data.pt')
    trans_output_path = os.path.join(output_path, 'processed_transactions.csv')
    laundering_accts_path = os.path.join(output_path, 'laundering_accounts.json')
    metadata_path = os.path.join(output_path, 'metadata.json')
    
    # å„²å­˜å¸³æˆ¶ ID æ˜ å°„
    with open(acct_id_path, 'w') as f:
        json.dump(acct_id, f, indent=4)
    
    # å„²å­˜æ´—éŒ¢å¸³æˆ¶æ¸…å–®
    with open(laundering_accts_path, 'w') as f:
        json.dump(list(laundering_accounts), f, indent=4)
    
    # å„²å­˜å…ƒæ•¸æ“šï¼ˆæ˜ å°„è¡¨ï¼‰
    metadata = {
        'currency_id': currency_id,
        'channel_id': channel_id,
        'num_nodes': len(acct_id),
        'num_edges': len(edge_index[0]),
        'num_laundering_accounts': labels.count(1),
        'edge_attr_description': [
            'is_self_txn',       # dim 0
            'txn_amt',           # dim 1
            'currency_id',       # dim 2
            'channel_id',        # dim 3
            'timestamp',         # dim 4
            'gfp_features'       # dim 5 onwards (åŒ…å«æ‰€æœ‰ GFP ç‰¹å¾µ)
        ],
        'edge_attr_total_dim': edge_attr.shape[1],
        'edge_attr_original_dim': 5,
        'edge_attr_gfp_dim': edge_gfp_features.shape[1],
        'gfp_time_window_days': 1
    }
    
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=4)
    
    # å„²å­˜ PyTorch Geometric åœ–è³‡æ–™
    torch.save(data, data_path)
    
    # å„²å­˜è™•ç†å¾Œçš„äº¤æ˜“è³‡æ–™
    trans_data.to_csv(trans_output_path, index=False)
    
    print(f"âœ“ å¸³æˆ¶ ID æ˜ å°„å·²å„²å­˜è‡³: {acct_id_path}")
    print(f"âœ“ æ´—éŒ¢å¸³æˆ¶æ¸…å–®å·²å„²å­˜è‡³: {laundering_accts_path}")
    print(f"âœ“ å…ƒæ•¸æ“šå·²å„²å­˜è‡³: {metadata_path}")
    print(f"âœ“ åœ–è³‡æ–™å·²å„²å­˜è‡³: {data_path}")
    print(f"âœ“ è™•ç†å¾Œçš„äº¤æ˜“è³‡æ–™å·²å„²å­˜è‡³: {trans_output_path}")
    print("\nå®Œæˆ!")


if __name__ == "__main__":
    main()